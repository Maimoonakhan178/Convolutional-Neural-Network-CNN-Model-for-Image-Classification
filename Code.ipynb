{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SET LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2.1: Load and preprocess the dataset\n",
    "def load_dataset(data_dir, image_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for category in os.listdir(data_dir):\n",
    "        category_dir = os.path.join(data_dir, category)\n",
    "        if os.path.isdir(category_dir):\n",
    "            for filename in os.listdir(category_dir):\n",
    "                if filename.endswith(\".jpg\"):\n",
    "                    img_path = os.path.join(category_dir, filename)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.equalizeHist(img)  # Apply histogram equalization\n",
    "                    img = cv2.resize(img, image_size)  # Resize images to a fixed size\n",
    "                    images.append(img)\n",
    "                    labels.append(category)\n",
    "                    print(f\"Loaded image: {img_path}, Label: {category}, Shape: {img.shape}\")\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHOWING THE NUMBER OF SAMPLES PRESENT IN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b1.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b11.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b12.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b13.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b14.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b15.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b2.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b3.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b4.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b5.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b6.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b7.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b8.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b9.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\Bv1.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv2.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv3.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv4.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv5.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f1.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f11.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f12.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f13.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f14.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f15.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f16.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f17.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f2.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f3.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f4.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f5.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f6.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f7.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f8.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f9.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv1.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv2.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv3.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv4.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv5.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\O1.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o12.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o14.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o15.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o2.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o4.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\Ov1.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov2.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov3.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov4.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov5.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p1.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p11.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p12.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p13.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p14.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p15.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p2.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p3.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p4.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p5.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p6.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\Pv1.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv2.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv3.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv4.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv5.jpg, Label: People2, Shape: (224, 224)\n",
      "Number of samples: 67\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'C:/Users/Maimoona Sabir/Desktop/CV'  \n",
    "X, y = load_dataset(data_dir)\n",
    "\n",
    "# Print the number of samples in the dataset\n",
    "print(f\"Number of samples: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BULDING AND TRAINING THE CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b1.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b11.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b12.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b13.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b14.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b15.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b2.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b3.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b4.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b5.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b6.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b7.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b8.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\b9.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\Bv1.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv2.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv3.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv4.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Buildings2\\bv5.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f1.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f11.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f12.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f13.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f14.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f15.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f16.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f17.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f2.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f3.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f4.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f5.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f6.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f7.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f8.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\f9.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv1.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv2.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv3.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv4.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Food2\\fv5.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\O1.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o12.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o14.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o15.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o2.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\o4.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\Ov1.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov2.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov3.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov4.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\Other2\\ov5.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p1.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p11.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p12.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p13.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p14.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p15.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p2.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p3.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p4.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p5.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\p6.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\Pv1.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv2.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv3.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv4.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV\\People2\\pv5.jpg, Label: People2, Shape: (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.2857 - loss: 8.5731 - val_accuracy: 0.1429 - val_loss: 396.1486\n",
      "Epoch 2/6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1562 - loss: 391.6403 - val_accuracy: 0.4286 - val_loss: 74.6762\n",
      "Epoch 3/6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4286 - val_loss: 74.6762\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1905 - loss: 85.2502 - val_accuracy: 0.2143 - val_loss: 11.7855\n",
      "Epoch 5/6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.2188 - loss: 10.1168 - val_accuracy: 0.2143 - val_loss: 6.6073\n",
      "Epoch 6/6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2143 - val_loss: 6.6073\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.2143 - loss: 6.6073\n",
      "Test Loss: 6.607332706451416, Test Accuracy: 0.2142857164144516\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000207A827ACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "Confusion Matrix:\n",
      "[[0 6 0 0]\n",
      " [0 3 0 0]\n",
      " [0 2 0 0]\n",
      " [0 3 0 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.21      1.00      0.35         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.21        14\n",
      "   macro avg       0.05      0.25      0.09        14\n",
      "weighted avg       0.05      0.21      0.08        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset directory exists\n",
    "if os.path.exists(data_dir) and len(os.listdir(data_dir)) > 0:\n",
    "    # Load and preprocess the dataset\n",
    "    X, y = load_dataset(data_dir)\n",
    "\n",
    "    # Check if dataset contains enough samples\n",
    "    if len(X) > 0:\n",
    "        # Split the dataset into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Reshape X_train and X_test to add the channel dimension\n",
    "        X_train = np.expand_dims(X_train, axis=-1)  # Assuming grayscale images\n",
    "        X_test = np.expand_dims(X_test, axis=-1)    # Assuming grayscale images\n",
    "        \n",
    "        # Convert labels to numerical format using LabelEncoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "        y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "        # Apply data augmentation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        train_generator = train_datagen.flow(X_train, y_train_encoded, batch_size=32)\n",
    "\n",
    "        # Build and compile the model\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),  # Adjust input shape to accept single-channel images\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dense(4, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model with data generator\n",
    "        history = model.fit(train_generator, \n",
    "                            steps_per_epoch=len(X_train) // 32, \n",
    "                            epochs=6, \n",
    "                            validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "        print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "        # Calculate confusion matrix and classification report\n",
    "        conf_matrix = confusion_matrix(y_test_encoded, y_pred)\n",
    "        class_report = classification_report(y_test_encoded, y_pred)\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Dataset contains no samples.\")\n",
    "else:\n",
    "    print(\"Error: Dataset directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION ON UNSEEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Buildings2\\b1.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Buildings2\\b2.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Buildings2\\b3.jpg, Label: Buildings2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Food2\\f1.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Food2\\f2.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Food2\\f3.jpg, Label: Food2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Other2\\O1.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Other2\\O2.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\Other2\\O3.jpg, Label: Other2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\People2\\p1.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\People2\\p2.jpg, Label: People2, Shape: (224, 224)\n",
      "Loaded image: C:/Users/Maimoona Sabir/Desktop/CV/Validation\\People2\\p3.jpg, Label: People2, Shape: (224, 224)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.2500 - loss: 6.5991\n",
      "Unseen Data Loss: 6.599142551422119, Unseen Data Accuracy: 0.25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "Confusion Matrix for Unseen Data:\n",
      "[[0 3 0 0]\n",
      " [0 3 0 0]\n",
      " [0 3 0 0]\n",
      " [0 3 0 0]]\n",
      "\n",
      "Classification Report for Unseen Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.25      1.00      0.40         3\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.25        12\n",
      "   macro avg       0.06      0.25      0.10        12\n",
      "weighted avg       0.06      0.25      0.10        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Maimoona Sabir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the unseen dataset\n",
    "unseen_data_dir = 'C:/Users/Maimoona Sabir/Desktop/CV/Validation'\n",
    "X_unseen, y_unseen = load_dataset(unseen_data_dir)\n",
    "\n",
    "# Check if the unseen dataset contains enough samples\n",
    "if len(X_unseen) > 0:\n",
    "    # Reshape X_unseen to add the channel dimension\n",
    "    X_unseen = np.expand_dims(X_unseen, axis=-1)  # Assuming grayscale images\n",
    "    \n",
    "    # Convert labels to numerical format using LabelEncoder\n",
    "    y_unseen_encoded = label_encoder.transform(y_unseen)\n",
    "\n",
    "    # Evaluate the model on the unseen dataset\n",
    "    unseen_loss, unseen_accuracy = model.evaluate(X_unseen, y_unseen_encoded)\n",
    "    print(f'Unseen Data Loss: {unseen_loss}, Unseen Data Accuracy: {unseen_accuracy}')\n",
    "    \n",
    "    # Generate predictions for the unseen data\n",
    "    y_unseen_pred = np.argmax(model.predict(X_unseen), axis=-1)\n",
    "\n",
    "    # Calculate confusion matrix and classification report for the unseen data\n",
    "    unseen_conf_matrix = confusion_matrix(y_unseen_encoded, y_unseen_pred)\n",
    "    unseen_class_report = classification_report(y_unseen_encoded, y_unseen_pred)\n",
    "\n",
    "    print(\"Confusion Matrix for Unseen Data:\")\n",
    "    print(unseen_conf_matrix)\n",
    "    print(\"\\nClassification Report for Unseen Data:\")\n",
    "    print(unseen_class_report)\n",
    "else:\n",
    "    print(\"Error: Unseen dataset contains no samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
